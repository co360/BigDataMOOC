{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "import week4 as w4\n",
    "sc.setLogLevel('DEBUG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del fichero CSV\n",
    "\n",
    "Vamos a cargar el fichero, que previamente hemos ingestado en HDFS, en un RDD de Spark, con el formato conveniente para nuestros objetivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('/user/cloudera/T_F_DR14_ZooSpec_10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos que hay 10.001 filas en el RDD. Esto quiere decir que incluye el header o cabecera\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos a desechar el header\n",
    "lines_f = lines.zipWithIndex().filter(lambda tup: tup[1] > 0).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convertimos las lineas de texto separado por comas en un DataFrame\n",
    "rows = lines_f.map(lambda l: l.split(\",\"))\n",
    "\n",
    "def build_features_bis(p):\n",
    "    return (p[0], int(p[1]), Vectors.dense([float(e) for e in p[2:]]),)\n",
    "\n",
    "data = rows.map(build_features_bis)\n",
    "df = sqlContext.createDataFrame(data, ['dr7objid', 'target', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un DataFrame es un objeto similar a una tabla con filas y columnas. En este caso, tiene 10.000 filas y tres columnas:\n",
    "* dr7objid: texto, id de la galaxia\n",
    "* target: número entero, tipo de galaxia\n",
    "* features: vector de 4096 posiciones con los píxeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos el número de filas y de columnas que tiene el DataFrame\n",
    "print('Número de filas (imágenes): {}'.format(df.count()))\n",
    "print('Número de columnas (id + target + features): {}'.format(len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de imágenes no clasificadas\n",
    "\n",
    "Para el entrenamiento no nos hacen falta todos los datos, nos basta con los datos de aquellas imágenes que se han clasificado satisfactoriamente. Así pues, vamos a descartar aquellas imágenes cuyo campo `target` tiene valor `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos cuántas imágenes hay de cada tipo\n",
    "# 0 = incierto\n",
    "# 1 = elíptica\n",
    "# 2 = espiral\n",
    "df.groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construímos un nuevo DataFrame solamente con las imágenes clasificadas\n",
    "labeled_df = df.filter(df['target'] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vemos que el número de imágenes seleccionadas es coherente con la query anterior\n",
    "labeled_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que valores toman los datos de este DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w4.describe_n(labeled_df, 4, 'features', 'feat_{0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de los datos: Principal Component Analysis (PCA)\n",
    "\n",
    "Los datos de los atributos, no es el ideal para entrenar un algoritmo de clasificación:\n",
    "* **es muy grande** 3701 filas * 4096 columnas ~ 15M de celdas\n",
    "* **es poco denso** hay pocas muestras (3701 imágenes) para el número de atributos (4096 píxeles). De intentar aplicar algunos algoritmos de clasificación sobre este conjunto de datos, podríamos incurrir en la [maldición de la dimensión (en inglés)](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n",
    "\n",
    "Para solucionar ambos problemas utilizaremos el método PCA para reducir el número de atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización de los datos\n",
    "\n",
    "Para poder aplicar la PCA, previamente tenemos que estandarizar los datos, eso es que todos los atributos esten centrados en 0 (tengan media 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_1 = StandardScaler(inputCol=\"features\", outputCol=\"std_features\",\n",
    "                        withStd=False, withMean=True)\n",
    "scalerModel_1 = scaler_1.fit(labeled_df)\n",
    "std_features = scalerModel_1.transform(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w4.describe_n(std_features, 4, 'std_features', 'std_feat_{0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de la PCA\n",
    "\n",
    "Ahora sí, aplicaremos la PCA.\n",
    "\n",
    "En este caso utilizaremos una implementación propia de la PCA, puesto que a la implementación que se puede encontrar en spark 1.6 le faltan algunas características que nos serán de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PCA as _pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `pca` nos devuelve tres objetos:\n",
    "* **comp**: numpy array de dimensiones n_features * k, con los componentes principales de nuestro set de datos\n",
    "* **score**: dataframe con la columna 'score' añadida, que contiene los k primeros coeficientes de los datos transformados al nuevo sistema de coordenadas\n",
    "* **eigVals**: numpy array de dimensión n_features, con valores propios de la matriz de covarianzas de nuestro set de datos\n",
    "\n",
    "Esta función puede tardar un rato: entre 10 y 15 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp, score, eigVals = _pca.pca(std_features, k=64, features_col='std_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número de componentes a conservar\n",
    "\n",
    "Vamos a ver qué porcentaje de información conservamos. Debemos seleccionar un número (k) lo más pequeño posible de \n",
    "componentes intentando conservar la mayor cantidad de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_atr = range(1, 1000, 5)\n",
    "info_perc = [_pca.info_perc(eigVals, i) for i in n_atr]\n",
    "plt.plot(n_atr, info_perc)\n",
    "plt.xlabel('Número de componentes seleccionados (k)')\n",
    "plt.ylabel('Porcentaje de información conservado')\n",
    "plt.axhline(95, color='k', linestyle='--')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos `k=64`, lo cual nos permite retener más del 95% de la información mientras que reducimos 64 veces el número de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_pca.info_perc(eigVals, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de los componentes conservados\n",
    "\n",
    "Los componentes conservados suelen capturar patrones significativos de los datos. Vemos en nuestro caso què son estos patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_comp_x = 5\n",
    "n_comp_y = 5\n",
    "g_size=1.\n",
    "fig = plt.figure(figsize=(g_size*n_comp_x, g_size*n_comp_y))\n",
    "for i in range(n_comp_x):\n",
    "    for j in range(n_comp_y):\n",
    "        comp_id = i*n_comp_y + j\n",
    "        ax = fig.add_subplot(n_comp_y, n_comp_x, comp_id + 1)\n",
    "        ax.imshow(comp[:, comp_id].reshape(64, 64), cmap='gist_heat')\n",
    "        ax.set_title(comp_id)\n",
    "        ax.axis('off')\n",
    "plt.tight_layout(pad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los componentes con mayor peso (aquellos con índices pequeños: 0, 1, 2, 3, ...) realmente contienen patrones semejantes a galaxias. A medida que seleccionamos componentes menos significativos, cada ves se asemejan menos a  patrones de galaxias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver qué valores toman los primeros atributos de los datos reducidos mediante PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w4.describe_n(score, 4, 'score', 'score_{0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabamos los datos en formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score.select('dr7objid', 'target', 'score').\\\n",
    "    write.save('pca_features.parquet', format='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, si abrimos un terminal y hacemos\n",
    "```\n",
    "$ hdfs dfs -ls -h\n",
    "```\n",
    "Veremos que el fichero parquet ya está en disco\n",
    "```\n",
    "-rw-r--r--   1 cloudera cloudera    605.0 M 2018-03-28 04:12 T_F_DR14_ZooSpec_10000.csv\n",
    "drwxr-xr-x   - cloudera cloudera          0 2018-05-04 02:21 pca_features.parquet\n",
    "```\n",
    "\n",
    "De hecho no es un fichero, sino un directorio con varios ficheros en su interior. También podemos ver el contenido de este directorio:\n",
    "```\n",
    "$ hdfs dfs -ls -h pca_features.parquet\n",
    "\n",
    "-rw-r--r--   1 cloudera cloudera          0 2018-05-04 02:21 pca_features.parquet/_SUCCESS\n",
    "-rw-r--r--   1 cloudera cloudera        982 2018-05-04 02:21 pca_features.parquet/_common_metadata\n",
    "-rw-r--r--   1 cloudera cloudera      4.6 K 2018-05-04 02:21 pca_features.parquet/_metadata\n",
    "-rw-r--r--   1 cloudera cloudera    378.5 K 2018-05-04 02:20 pca_features.parquet/part-r-00000-43a02924-fb62-4858-8f29-876abd66795b.gz.parquet\n",
    "-rw-r--r--   1 cloudera cloudera    374.5 K 2018-05-04 02:21 pca_features.parquet/part-r-00001-43a02924-fb62-4858-8f29-876abd66795b.gz.parquet\n",
    "-rw-r--r--   1 cloudera cloudera    386.3 K 2018-05-04 02:21 pca_features.parquet/part-r-00002-43a02924-fb62-4858-8f29-876abd66795b.gz.parquet\n",
    "-rw-r--r--   1 cloudera cloudera    398.5 K 2018-05-04 02:21 pca_features.parquet/part-r-00003-43a02924-fb62-4858-8f29-876abd66795b.gz.parquet\n",
    "-rw-r--r--   1 cloudera cloudera    274.8 K 2018-05-04 02:21 pca_features.parquet/part-r-00004-43a02924-fb62-4858-8f29-876abd66795b.gz.parquet\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la próxima lección veremos como entrenar un algoritmo de clasificación con estos datos.\n",
    "\n",
    "Gracias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
